---
title: Testing
layout: default
output: bookdown::html_chapter
---

```{r, echo = FALSE}
library(testthat)
```

# 테스팅(Testing) {#tests}

테스팅은 팩키지 개발에서 있어 매우 중요한 부분이다.
테스팅을 통해서 작성한 코드가 의도한 대로 동작하는 것을 확신할 수 있다.
그런 장점에도 불구하구, 테스팅은 개발흐름에 부가 단계를 추가한다.
이번 장 목표는 `testthat` 팩키지를 사용해서 정형적인 테스팅 자동화를 통해서 테스팅 작업을 좀더 쉽고 효과적으로 수행하는 방법을 시연하는데 있다.

지금까지 작업흐름은 아마도 다음과 같을 것이다:

1. 함수를 작성한다.
1. Ctrl/Cmd + Shift + L 단축키 혹은 `devtools::load_all()` 명령어를 통해서 함수를 적재한다.
1. 작성한 함수가 동작하는지 살펴보기 위해서 콘솔에서 실험한다.
1. 함수를 깔끔하게 만들고, 과정을 반복한다.

작업흐름 과정에서 코드를 _테스팅_을 했지만, 비정형으로만 작업을 수행한 것이다.
이러한 접근방법의 문제점은 3개월이 지난 뒤에 코드 베이스에 신규 기능을 추가하려면, 개발 초기 실행했던
비정형 테스트 일부를 아마도 잊어버렸을 것이다.
따라서, 잘 동작했던 코드가 깨져서 돌아기지 않기 매우 쉽게 된다.

저자가 테스트 자동화를 사용하기 시작한 이유는 너무 많은 시간을 이전에 고친 버그를 다시 고치는데 쓰고 있다는 것을 발견했기 때문이다.
코드를 작성하고 버그를 고치면서 작성한 코드가 잘 동작하는지 확인하는데 인터랙티브 테스트를 수행한다.
하지만, 저자는 테스트를 저장할 수 있는 시스템을 갖추지 않았다. 그래서, 필요해지면 테스트를 다시 실행할 수 있었다. 저자 생각에는 R 프로그래머 사이에 이러한 것이 일반적이라고 본다.
이유는 코드를 테스트하지 않기 때문이 아니라, 테스트를 자동화하지 않았기 때문이다.

이번 장에서 명령라인에서 수행하는 비정형 무작위 테스팅을 졸업하고, 정형화된 테스트 자동화(aka, 단위 테스트)로 옮겨가는 것을 학습한다.
임시방편 인터랙티브 테스트를 재현가능 스크립트로 변환하는 것이 앞단에 일부 업무부하를 주지만, 4가지 방식으로 나중에 보상을 한다:

* 더 적은 버그. 코드가 동작해야 되는 방식에 대해 명시적으로 선언했기 때문에,
  더 적은 버그를 갖게 된다. 이유는 회계학에서 복식부기가 동작하는 것과 유사하다:
  코드와 테스트 두곳에 작성한 코드 행동을 기술했기 때문에, 서로에 대해 검증할 수 있다.
  이러한 접근방법을 테스팅에 도입하면, 
  과거에 고친 버그가 결코 귀신처럼 다시 쫓아다니지 못하게 확실히 할 수 있다.
  
* 향상된 코드 구조. 테스트하기 쉬운 코드는 일반적으로 더 잘 설계되었다.
  이유는 테스트를 작성하면 어쩔 수 없이 코드의 복잡한 부분을 격리되어 동작되는 별도 함수로 쪼개야 
  되기 때문이다. 코드에 중복도 줄여준다.
  결과적으로 함수는 테스트하고, 이해하고, 동작하기 쉽게 된다
  (새로운 방식으로 조합하기도 더 쉽게 된다).
  

* 더 쉬운 재시작. (예를 들어, 구현하려는 다음 기능에 대해서) 
  항상 실패 테스트(failing test)를 생성해서 코딩 세션을 마무리한다면, 
  테스팅을 통해서 마지막 중단한 곳을 쉽게 찾아준다: 
  테스트를 통해서 다음 무엇을 수행해야 하는지 알게 해준다.
  

* 강건한 코드. 작성한 팩키지의 거의 모든 주요 기능이 연관된 테스트를 갖는 것을 알게 되면,
  신뢰감을 갖고 우연히 무언가 망가뜨린다는 걱정없이 큰 변화를 줄 수 있다.
  작업을 수행할 훨씬 더 단순한 방법이 생각났을 때 저자에게 이 기능이 유용하다.
  (대체로 해결책이 훨씬 더 단순한 이유는 중요한 사용자 사용사례(use case)를 잊어버렸다는 것이다!).

만약 이미 다른 프로그래밍 언어 단위 테스트에 친숙하다면,
`testthat`에 일부 근본적인 차이가 있음을 주목한다.
이유는 R 자체가 가슴속으로 객체지향 프로그래밍이라기 보다는 함수형 프로그래밍 언어에 가깝기 때문이다.
예를 들어, R 주요 객체지향 시스템 (S3와 S4)는 제네릭 함수에 기반하기 때문에 (즉, 메쏘드는 클래스가 아닌 함수에 속해있다), 객체와 메쏘드로 구성된 테스팅 접근법이 그다지 의미를 갖지 못한다.


## 테스트 작업흐름 {#test-workflow}

`testhat`을 사용하는 팩키지를 설정하려면, 다음을 실행한다:

```{r, eval = FALSE}
devtools::use_testthat()
```

상기 명령어는 다음을 수행한다:

1.  `tests/testthat` 디렉토리를 생성한다.

1.  testthat을 `DESCRIPTION` 파일 `Suggests` 필드에 추가한다.

1.  `tests/testthat.R` 파일을 생성해서 `R CMD check`을 실행할 때 모든 테스트를 실행한다.
     ([automated checking](#check)에서 좀더 학습할 것이다.)


설정만 마치게되면, 작업흐름은 단순하다:


1.  코드 혹은 테스트를 변형한다.

2.  Ctrl/Cmd + Shift + T 단축키 혹은 `devtools::test()` 명령어로 팩키지를 테스트한다.

3.  모든 테스트가 통과할 때까지 반복한다.

테스팅 출력결과는 다음과 같아 보인다:

    Expectation : ...........
    rv : ...
    Variance : ....123.45.

각 행은 테스트 파일을 표현한다.
각 `.` 기호는 통과한 테스트를 표현한다.
각 숫자는 실패한 테스트를 표현한다.
세부 정보를 담고 있는 실패 목록으로 숫자는 색인된다:


    1. Failure(@test-variance.R#22): Variance correct for discrete uniform rvs -----
    VAR(dunif(0, 10)) not equal to var_dunif(0, 10)
    Mean relative difference: 3
    
    2. Failure(@test-variance.R#23): Variance correct for discrete uniform rvs -----
    VAR(dunif(0, 100)) not equal to var_dunif(0, 100)
    Mean relative difference: 3.882353

테스트 각 실패는 테스트에 대한 기술 (예를 들어, "Variance correct for discrete uniform rvs"),
위치 (예를 들어, "\@test-variance.R#22"), 실패로 돌아간 사유 (예를 들어, "VAR(dunif(0, 10)) not equal to var_dunif(0, 10)") 정보를 전달한다. 목표는 모든 테스트를 통과시키는 것이다.

## 테스트 구조 {#test-structure}

테스트 파일은 `tests/testthat/`에 위치한다.
테스트 파일 명칭은 `test`로 시작해야 된다.
`stringr` 팩키지에서 나온 테스트 파일 예제가 다음에 있다:


```{r}
library(stringr)
context("String length")

test_that("str_length is number of characters", {
  expect_equal(str_length("a"), 1)
  expect_equal(str_length("ab"), 2)
  expect_equal(str_length("abc"), 3)
})

test_that("str_length of factor is length of level", {
  expect_equal(str_length(factor("a")), 1)
  expect_equal(str_length(factor("ab")), 2)
  expect_equal(str_length(factor("abc")), 3)
})

test_that("str_length of missing is missing", {
  expect_equal(str_length(NA), NA_integer_)
  expect_equal(str_length(c(NA, 1)), c(NA, 1))
  expect_equal(str_length("NA"), 2)
})
```

테스트는 계층적으로 구성되어 있다: __expectations__는 __tests__ 집단으로 묶여지고, __tests__는 __files__ 집단으로 묶여진다:


* __예상(expectation)__이 테스팅 원자에 해당된다. 
  연산에 대한 예상결과를 기술한다: 올바른 값과 올바른 클래스를 갖는가?
  오류 메시지를 내야할 때, 오류 메시지를 생성하는가?
  예상(expectation)은 시각적 점검하는 콘솔 결과값을 자동화한다.
  예상(expectation)은 `expect_`으로 시작하는 함수가 된다.
  

* __테스트(test)__는 예상(expectation) 다수를 집단으로 묶어 간단한 함수에서 출력결과를 테스트한다.
  좀더 복잡한 함수에서 단일 매개변수에 대한 가능한 범위 혹은 함수 다수로부터 단단히 연관된 기능이 포함된다.
  이것이 종종 한 기능 단위를 테스트할 때 __단위(unit)__라고 불리는 이유다.
  테스트는 `test_that()`으로 생성된다.

* __파일(file)__은 연관된 테스트 다수를 집단으로 묶는다. 
  파일은 `context()` 명령어로 사람이 읽을 수 있는 명칭이 부여된다.

다음에 각각에 대해 자세한 사항이 기술된다.

### 예상(Expectations)

예상은 가장 낮은 수준 테스팅으로 함수 호출이 기대한 바를 수행하는지 아닌지에 대해 이진 가정대입(binary assertion)한다. 모든 예상은 유사한 구조를 갖느다:

* `expect_` 으로 시작한다.

* 인자를 두개 갖는다: 첫번째는 실제 결과, 두번째는 예상하는 것.

* 만약 실제 결과값과 예상 결과값이 일치하지 않으면, `testthat`은 오류를 던진다.

정상적으로 파일 내부, 테스트 내부에 예상을 넣지만, 
직접적으로도 예상을 실행할 수 있다.
인터랙티브하게 예상을 탐색하기 쉽게 만든다.
`testthat` 팩키지에는 거의 20개 예상이 있다.
가장 중요한 것이 아래 논의된다.


*   등치를 테스트하는 기본적인 방법이 두개 있다: `expect_equal()`, `expect_identical()`.
    `expect_equal()`이 가장 흔히 사용된다: `all.equal()`을 사용해서 수치해석 허용한계에서 
    등치성을 점검한다:

    ```{r, error = TRUE}
    expect_equal(10, 10)
    expect_equal(10, 10 + 1e-7)
    expect_equal(10, 11)
    ```
  
    만약 정확한 등치성을 시험하거나, 환경(environment)같은 외래 객체와 비교가 필요하다면,
    `expect_identical()`을 사용한다. 이 함수는 `identical()` 위에서 빌드되었다:

    ```{r, error = TRUE}
    expect_equal(10, 10 + 1e-7)
    expect_identical(10, 10 + 1e-7)
    ```

*   `expect_match()`는 정규표현식에 문자 벡터를 매칭한다.
    선택옵션 `all` 인자를 통해 모든 요소 혹은 단지 요소 하나만 매칭할지를 제어한다.
    `grepl()` 명령어로 강력한 기능이 부여된다 (`ignore.case = FALSE` 혹은 `fixed = TRUE` 같은
    부가 인자가 매개변수로 전달된다).


    ```{r, error = TRUE}
    string <- "Testing is fun!"

    expect_match(string, "Testing") 
    # Fails, match is case-sensitive
    expect_match(string, "testing")

    # Additional arguments are passed to grepl:
    expect_match(string, "testing", ignore.case = TRUE)
    ```

*   4가지 `expect_match()` 변형 명령어로 다른 결과 유형을 점검한다:
    `expect_output()`은 출력결과를 점검한다; `expect_message()`는 메시지를 점검한다;
    `expect_warning()`은 경고를 점검한다; `expect_error()`은 오류를 점검한다.


    ```{r, error = TRUE}
    a <- list(1:10, letters)

    expect_output(str(a), "List of 2")
    expect_output(str(a), "int [1:10]", fixed = TRUE)

    expect_message(library(mgcv), "This is mgcv")
    ```

    `expect_message()`, `expect_warning()`, `expect_error()` 명령어에 두번째 인자를 공백으로 남겨두면,
    메시지, 경고, 오류가 생성되었는지만 확인한다.
    하지만, 일반적으로 명시적으로 메시지로부터 테스트 일부를 제공하는 것이 더 낫다.
    
    
    ```{r, error = TRUE}  
    expect_warning(log(-1))
    expect_error(1 / "a") 

    # But always better to be explicit
    expect_warning(log(-1), "NaNs produced")
    expect_error(1 / "a", "non-numeric argument")

    # Failure to produce a warning or error when expected is an error
    expect_warning(log(0))
    expect_error(1 / 2) 
    ```

*   `expect_is()` 명령어를 통해서 특정된 클래스에서 객체가 `inherit()` 상속되었는지 점검한다.

    ```{r, error = TRUE}
    model <- lm(mpg ~ wt, data = mtcars)
    expect_is(model, "lm")
    expect_is(model, "glm")
    ```

*   만약 어떤 예상도 필요한 것을 수행하지 않는다면, `expect_true()` 와 `expect_false()`이 유용하다.

*   때때로, 결과가 무엇이 되어야 하는지 알지 못하거나, 혹은 너무 복잡해서 코드로 재생성하기가 쉽지 않다.
    이런 경우에, 취할 수 있는 가장 최선은 결과가 가장 마지막 수행결과와 동일한지 점검하는 것이다.
    `expect_equal_to_reference()`으로 첫 실행 결과를 캐쉬로 갖고 나서, 후속 실행결과와 비교한다.
    만약 어떤 연유로 인해서 결과가 변경되면, 캐쉬 (*) 파일을 삭제만하고 다시 테스트한다.

일련의 예상을 실행하는 것이 유용한데 이유는 작성한 코드가 예상한대로 동작하는지 확실히 해주기 때문이다.
심지어 함수 내부에도 예상을 사용해서 입력값이 기대한 것인지 점검한다.
하지만, 뭔가 잘못되면 그다지 유용하지는 않다.
이를 통해 무언가 기대한 것이 아니라는 것만 알게 된다. 예상 목적을 알지 못하기 때문이다.
다음에 기술되는 테스트는 예상을 일관된 블록을 구조화해서 전반적인 예상 집합 목적을 기술한다.


## 테스트 작성 {#test-tests}

각 테스트는 정보가 되는 명칭을 가져야 되고, 단일 기능 단위만을 다뤄야만 된다.
기본적인 생각은 테스트가 실패할 때, 무엇이 잘못되었는지와 코드에서 문제점이 어디에 있는지 찾을 수 있게 한다.
테스트 명칭과 코드 블록을 인자로 갖는 `test_that()` 명령어를 사용해서 신규 테스트를 생성한다.
테스트 명칭은 "Test that ..." 완전한 문장이 되어야 한다. 코드 블록은 예상 집합이다.

예상을 테스트로 어떻게 구조화할지는 작성자에 달려있다.
주요한 점은 테스트와 연관된 메시지가 정보가 되게 작성되어 문제 원천으로 빠르게 좁혀갈 수 있어야 된다.
한 테스트에 너무 많은 예상을 넣는 것을 피하라 - 좀더 커다란 적은 수의 테스트 보다는 더 많은 작은 테스트를 갖는게 더 낫다.


각 테스트는 자체 환경에서 실행되고 그 자체로 완비되어야 한다.
하지만, 동작(action)이 R 실행환경에 영향을 준 다음에 정리하는 방법을 `testthat`은 갖고 있지 않다: 


* 파일시스템: 파일 생성과삭제, 작업 디렉토리 변경 등.

* 검색 경로: `library()`, `attach()`.

* `options()` 와 `par()` 같은 전역 선택옵션.

테스트에 상기 동작(action)을 사용하면, 본인 스스로 정리할 필요가 있다.

다른 많은 테스팅 팩키지가 설정 및 해제 메쏘드를 갖고서 각 테스트 이전과 이후에 자동으로 실행되지만, 
`testthat` 팩키지에서는 그다지 중요하지 않다. 이유는 테스트 외부에 객체를 생성하고, 
R의 '변경에 복사(copy-on-modify)' 시맨틱을 사용해서 테스트 실행 간에 변경이 일어나지 않게 한다.
다른 동작을 정리하는데는 정규 R 함수를 사용한다.


### 테스트 대상 (What to test)

> 무언가를 출력문장(print statement) 혹은 디버거 표현식으로 타이핑하려는 유혹이 있을 때마다,
> 그러지 말고 대신에 테스트로 작성하라. --- 마틴 파울러(Martin Fowler)

테스트 

There is a fine balance to writing tests. Each test that you write makes your code less likely to change inadvertently; but it also can make it harder to change your code on purpose. It's hard to give good general advice about writing tests, but you might find these points helpful:

* Focus on testing the external interface to your functions - if you test the 
  internal interface, then it's harder to change the implementation in the 
  future because as well as modifying the code, you'll also need to update all 
  the tests.

* Strive to test each behaviour in one and only one test. Then if that 
  behaviour later changes you only need to update a single test.

* Avoid testing simple code that you're confident will work. Instead focus your 
  time on code that you're not sure about, is fragile, or has complicated 
  interdependencies. That said, I often find I make the most mistakes when I 
  falsely assume that the problem is simple and doesn't need any tests.

* Always write a test when you discover a bug. You may find it helpful to adopt 
  the test-first philosphy. There you always start by writing the tests, and 
  then write the code that makes them pass. This reflects an important problem
  solving strategy: start by establishing your success critieria, how you
  know if you've solved the problem.

### Skipping a test

Sometimes it's impossible to perform a test - you may not have an internet connection or you may be missing an important file. Unfortunately, another likely reason follows from this simple rule: the more machines you use to write your code, the more likely it is that you won't be able to run all of your tests. In short, there are times when, instead of getting a failure, you just want to skip a test. To do that, you can use the `skip()` function - rather than throwing an error it simply prints an `S` in the output.

```{r, eval = FALSE}
check_api <- function() {
  if (not_working()) {
    skip("API not available")
  }
}

test_that("foo api returns bar when given baz", {
  check_api()
  ...
})
```

### Building your own testing tools

As you start to write more tests, you might notice duplication in your code. For example, the following code shows one test of the `floor_date()` function from `library(lubridate)`. There are seven expectations that check the results of rounding a date down to the nearest second, minute, hour, etc. There's a lot of duplication (which increases the chance of bugs), so we might want to extract common behaviour into a new function.

```{r}
library(lubridate)
test_that("floor_date works for different units", {
  base <- as.POSIXct("2009-08-03 12:01:59.23", tz = "UTC")

  expect_equal(floor_date(base, "second"), 
    as.POSIXct("2009-08-03 12:01:59", tz = "UTC"))
  expect_equal(floor_date(base, "minute"), 
    as.POSIXct("2009-08-03 12:01:00", tz = "UTC"))
  expect_equal(floor_date(base, "hour"),   
    as.POSIXct("2009-08-03 12:00:00", tz = "UTC"))
  expect_equal(floor_date(base, "day"),    
    as.POSIXct("2009-08-03 00:00:00", tz = "UTC"))
  expect_equal(floor_date(base, "week"),   
    as.POSIXct("2009-08-02 00:00:00", tz = "UTC"))
  expect_equal(floor_date(base, "month"),  
    as.POSIXct("2009-08-01 00:00:00", tz = "UTC"))
  expect_equal(floor_date(base, "year"),   
    as.POSIXct("2009-01-01 00:00:00", tz = "UTC"))
})
```

I'd start by defining a couple of helper functions to make each expectation more concise. That allows each test to fit on one line, so you can line up actual and expected values to make it easier to see the differences:

```{r}
test_that("floor_date works for different units", {
  base <- as.POSIXct("2009-08-03 12:01:59.23", tz = "UTC")
  floor_base <- function(unit) floor_date(base, unit)
  as_time <- function(x) as.POSIXct(x, tz = "UTC")

  expect_equal(floor_base("second"), as_time("2009-08-03 12:01:59"))
  expect_equal(floor_base("minute"), as_time("2009-08-03 12:01:00"))
  expect_equal(floor_base("hour"),   as_time("2009-08-03 12:00:00"))
  expect_equal(floor_base("day"),    as_time("2009-08-03 00:00:00"))
  expect_equal(floor_base("week"),   as_time("2009-08-02 00:00:00"))
  expect_equal(floor_base("month"),  as_time("2009-08-01 00:00:00"))
  expect_equal(floor_base("year"),   as_time("2009-01-01 00:00:00"))
})
```

We could go a step further and create a custom expectation function:

```{r}
base <- as.POSIXct("2009-08-03 12:01:59.23", tz = "UTC")

expect_floor_equal <- function(unit, time) {
  expect_equal(floor_date(base, unit), as.POSIXct(time, tz = "UTC"))
}
expect_floor_equal("year", "2009-01-01 00:00:00")
```

However, if the expectation fails this doesn't give very informative output:

```{r, error = TRUE}
expect_floor_equal("year", "2008-01-01 00:00:00")
```

Instead you can use a little [non-standard evaluation](http://adv-r.had.co.nz/Computing-on-the-language.html) to produce something more informative. The key is to use `bquote()` and `eval()`. In the `bquote()` call below, note the use of `.(x)` - the contents of `()` will be inserted into the call.

```{r, error = TRUE}
expect_floor_equal <- function(unit, time) {
  as_time <- function(x) as.POSIXct(x, tz = "UTC")
  eval(bquote(expect_equal(floor_date(base, .(unit)), as_time(.(time)))))
}
expect_floor_equal("year", "2008-01-01 00:00:00")
```

This sort of refactoring is often worthwhile because removing redundant code makes it easier to see what's changing. Readable tests give you more confidence that they're correct.

```{r}
test_that("floor_date works for different units", {
  as_time <- function(x) as.POSIXct(x, tz = "UTC")
  expect_floor_equal <- function(unit, time) {
    eval(bquote(expect_equal(floor_date(base, .(unit)), as_time(.(time)))))
  }

  base <- as_time("2009-08-03 12:01:59.23")
  expect_floor_equal("second", "2009-08-03 12:01:59")
  expect_floor_equal("minute", "2009-08-03 12:01:00")
  expect_floor_equal("hour",   "2009-08-03 12:00:00")
  expect_floor_equal("day",    "2009-08-03 00:00:00")
  expect_floor_equal("week",   "2009-08-02 00:00:00")
  expect_floor_equal("month",  "2009-08-01 00:00:00")
  expect_floor_equal("year",   "2009-01-01 00:00:00")
})
```

## Test files {#test-files}

The highest-level structure of tests is the file. Each file should contain a single `context()` call that provides a brief description of its contents. Just like the files in the `R/` directory, you are free to organise your tests any way that you like. But again, the two extremes are clearly bad (all tests in one file, one file per test). You need to find a happy medium that works for you. A good starting place is to have one file of tests for each complicated function.

## CRAN notes {#test-cran}

CRAN will run your tests on all CRAN platforms: Windows, Mac, Linux and Solaris. There are a few things to bear in mind:

* Tests need to run relatively quickly - aim for under a minute. Place 
  `skip_on_cran()` at the beginning of long-running tests that shouldn't be run 
  on CRAN - they'll still be run locally, but not on CRAN.

* Note that tests are always run in the English language (`LANGUAGE=EN`) and
  with C sort order (`LC_COLLATE=C`). This minimises spurious differences
  between platforms.

* Be careful about testing things that are likely to be variable on CRAN 
  machines. It's risky to test how long something takes (because CRAN machines
  are often heavily loaded) or to test parallel code (because CRAN runs multiple 
  package tests in parallel, multiple cores will not always available). 
  Numerical precision can also vary across platforms (it's often less precise on 
  32-bit versions of R) so use `expect_equal()` rather than `expect_identical()`.

[tdd]:http://en.wikipedia.org/wiki/Test-driven_development
[extreme-programming]:http://en.wikipedia.org/wiki/Extreme_programming
